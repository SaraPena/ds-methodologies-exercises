{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "In this lesson, we will acquire and prepare the data we will use in the rest of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "\n",
    "Spark lets us read data in from a variety of data sources using what is called a `DataFrameReader`. We can access the `read` property of our `spark` object and then set various options and read from a data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('data/source.csv', sep = \",\", header=True, inferSchema=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code could also be written like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format('csv')\n",
    "    .option('sep', ',')\n",
    "    .option('inferSchema', True)\n",
    "    .option('header', True)\n",
    "    .load('data/source.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Schemas\n",
    "\n",
    "Spark includes a concept of a data schema, which is a way to specify the types of our data ahead of time. Doing so lets us be sure about the structure of our data, and can significantly increase the speed of loading data (inferring the schema can bea costly opteration for large datasets).\n",
    "\n",
    "We'll import several things from the `pyspark.sql.types` module:\n",
    "- `StringType`\n",
    "- `DoubleType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `ShortType`\n",
    "- `TimestampType`\n",
    "- `FloatType`\n",
    "- `DataType`\n",
    "\n",
    "All of the above types will go inside the `StructField`, which will be encapsulated in the `StructType`, and the resulting object will represent our data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('source_id', StringType()),\n",
    "        StructField('source_username', StringType())\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "spark.read.csv('data/source.csv', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that instead of `inferSchema = True`, we pass the `schema` object in to the `read.csv` call.\n",
    "\n",
    "# Writing Data\n",
    "\n",
    "A spark dataframe can written to a local destination using the `.write` property. Several common output formats are:\n",
    "- `.csv`: for writing to a local csv file(s)\n",
    "- `.parquet`: Parquet is a very popular columnar storgag format for Hadoop.\n",
    "- `.json`: for writing a local json file(s)\n",
    "- `.jdbc`: for writing to a SQL database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|                a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|                a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|                a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|                a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|                a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "|        audi|                a4|  2.8|1999|  6|manual(m5)|  f| 18| 26|  p|compact|\n",
      "|        audi|                a4|  3.1|2008|  6|  auto(av)|  f| 18| 27|  p|compact|\n",
      "|        audi|        a4 quattro|  1.8|1999|  4|manual(m5)|  4| 18| 26|  p|compact|\n",
      "|        audi|        a4 quattro|  1.8|1999|  4|  auto(l5)|  4| 16| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  2.0|2008|  4|manual(m6)|  4| 20| 28|  p|compact|\n",
      "|        audi|        a4 quattro|  2.0|2008|  4|  auto(s6)|  4| 19| 27|  p|compact|\n",
      "|        audi|        a4 quattro|  2.8|1999|  6|  auto(l5)|  4| 15| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  2.8|1999|  6|manual(m5)|  4| 17| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  3.1|2008|  6|  auto(s6)|  4| 17| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  3.1|2008|  6|manual(m6)|  4| 15| 25|  p|compact|\n",
      "|        audi|        a6 quattro|  2.8|1999|  6|  auto(l5)|  4| 15| 24|  p|midsize|\n",
      "|        audi|        a6 quattro|  3.1|2008|  6|  auto(s6)|  4| 17| 25|  p|midsize|\n",
      "|        audi|        a6 quattro|  4.2|2008|  8|  auto(s6)|  4| 16| 23|  p|midsize|\n",
      "|   chevrolet|c1500 suburban 2wd|  5.3|2008|  8|  auto(l4)|  r| 14| 20|  r|    suv|\n",
      "|   chevrolet|c1500 suburban 2wd|  5.3|2008|  8|  auto(l4)|  r| 11| 15|  e|    suv|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for demo purposes\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data('mpg'))\n",
    "mpg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.write.json('data/mpg_json', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like much else in spark, there's multiple ways we could do this:\n",
    "\n",
    "(\n",
    "    mpg.write.format('csv')\n",
    "    .mode('overwrite')\n",
    "    .option('header', 'true')\n",
    "    .save('data/mpg_csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "For the rest of this lesson, we'll take a look at the `case` data from the San Antonio 311 calls dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, SLA_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('data/case.csv', header = True, inferSchema = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 1/1/18 0:42          \n",
      " case_closed_date     | 1/1/18 12:29         \n",
      " SLA_due_date         | 9/26/20 0:42         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 1/1/18 0:46          \n",
      " case_closed_date     | 1/3/18 8:11          \n",
      " SLA_due_date         | 1/5/18 8:30          \n",
      " case_late            | NO                   \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | YES                  \n",
      " dept_division        | Storm Water          \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  GOLIAD RD, ... \n",
      " council_district     | 3                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now cover various pieces of data preparation we wish to do and how to do those with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns\n",
    "\n",
    "We'll rename this column to match with the other data-type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 1/1/18 0:42          \n",
      " case_closed_date     | 1/1/18 12:29         \n",
      " case_due_date        | 9/26/20 0:42         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 1/1/18 0:46          \n",
      " case_closed_date     | 1/3/18 8:11          \n",
      " case_due_date        | 1/5/18 8:30          \n",
      " case_late            | NO                   \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | YES                  \n",
      " dept_division        | Storm Water          \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  GOLIAD RD, ... \n",
      " council_district     | 3                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('SLA_due_date', 'case_due_date')\n",
    "df.show(2, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Data Types\n",
    "\n",
    "Two columns, `case_closed` and `case_late` store yes/no values. Currently spark thinks they are strings; let's turn them into booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|case_closed|case_late| count|\n",
      "+-----------+---------+------+\n",
      "|         NO|      YES|  6525|\n",
      "|        YES|      YES| 87978|\n",
      "|         NO|       NO| 11585|\n",
      "|        YES|       NO|735616|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demonstraing we only have yes/no in each field\n",
    "\n",
    "df.groupBy('case_closed', 'case_late').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|     true|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('case_closed', expr('case_closed == \"YES\"')).withColumn('case_late', expr('case_late == \"YES\"'))\n",
    "df.select('case_closed', 'case_late').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `council_district` column appears as though it is an integer, but this is just a unique identifier for each district, that is, we aren't going to be performing arithmetic with this number, so we will turn it into a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|council_district| count|\n",
      "+----------------+------+\n",
      "|               1|119309|\n",
      "|               6| 74095|\n",
      "|               3|102706|\n",
      "|               5|114609|\n",
      "|               9| 40916|\n",
      "|               4| 93778|\n",
      "|               8| 42345|\n",
      "|               7| 72445|\n",
      "|              10| 62926|\n",
      "|               2|114745|\n",
      "|               0|  3830|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('council_district').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('council_district', col('council_district').cast('string'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will handle the 3 column that have dates in them. We'll use spark's `to_timestamp` function for this.\n",
    "\n",
    "In order to work properly, we'll need to provide the date format when using `to_timestamp`. The date format is a little different than the date functionality we've worked with in pandas, this is because it is using ***Java's SimpleDateFormat.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Before handling dates\n",
      "+----------------+----------------+-------------+\n",
      "|case_opened_date|case_closed_date|case_due_date|\n",
      "+----------------+----------------+-------------+\n",
      "|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|\n",
      "|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|\n",
      "|     1/1/18 0:48|     1/2/18 7:57|  1/5/18 8:30|\n",
      "|     1/1/18 1:29|     1/2/18 8:13| 1/17/18 8:30|\n",
      "|     1/1/18 1:34|    1/1/18 13:29|  1/1/18 4:34|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--Before handling dates\")\n",
    "df.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M/d/yy H:mm'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt = \"M/d/yy H:mm\"\n",
    "fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: timestamp, case_closed_date: timestamp, case_due_date: timestamp, case_late: boolean, num_days_late: double, case_closed: boolean, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    df.withColumn('case_opened_date', to_timestamp('case_opened_date',fmt))\n",
    "    .withColumn('case_closed_date', to_timestamp('case_closed_date', fmt))\n",
    "    .withColumn('case_due_date', to_timestamp('case_due_date', fmt))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- After\n",
      "+-------------------+-------------------+-------------------+\n",
      "|   case_opened_date|   case_closed_date|      case_due_date|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|\n",
      "|2018-01-01 00:46:00|2018-01-03 08:11:00|2018-01-05 08:30:00|\n",
      "|2018-01-01 00:48:00|2018-01-02 07:57:00|2018-01-05 08:30:00|\n",
      "|2018-01-01 01:29:00|2018-01-02 08:13:00|2018-01-17 08:30:00|\n",
      "|2018-01-01 01:34:00|2018-01-01 13:29:00|2018-01-01 04:34:00|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('--- After')\n",
    "df.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations\n",
    "\n",
    "Now that we have everything stored as the correct data type, we will make a few transformations to the data.\n",
    "\n",
    "We'll begin by normalizing the request address field. Using the `trim` and `lower` functions lets us strip any leading or trailing whitespace and convert everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Before\n",
      "+-------------------------------------+\n",
      "|request_address                      |\n",
      "+-------------------------------------+\n",
      "|2315  EL PASO ST, San Antonio, 78207 |\n",
      "|2215  GOLIAD RD, San Antonio, 78223  |\n",
      "|102  PALFREY ST W, San Antonio, 78223|\n",
      "|114  LA GARDE ST, San Antonio, 78223 |\n",
      "|734  CLEARVIEW DR, San Antonio, 78228|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('---Before')\n",
    "df.select('request_address').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: timestamp, case_closed_date: timestamp, case_due_date: timestamp, case_late: boolean, num_days_late: double, case_closed: boolean, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn('request_address', trim(lower(df.request_address)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---After\n",
      "+-------------------------------------+\n",
      "|request_address                      |\n",
      "+-------------------------------------+\n",
      "|2315  el paso st, san antonio, 78207 |\n",
      "|2215  goliad rd, san antonio, 78223  |\n",
      "|102  palfrey st w, san antonio, 78223|\n",
      "|114  la garde st, san antonio, 78223 |\n",
      "|734  clearview dr, san antonio, 78228|\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('---After')\n",
    "df.select('request_address').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will convert the number of days a case is late to a number of weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('num_weeks_late', expr('num_days_late/7 AS num_weeks_late'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      num_days_late|      num_weeks_late|\n",
      "+-------------------+--------------------+\n",
      "| -998.5087616000001|        -142.6441088|\n",
      "|-2.0126041669999997|-0.28751488099999994|\n",
      "|       -3.022337963|-0.43176256614285713|\n",
      "|       -15.01148148| -2.1444973542857144|\n",
      "|0.37216435200000003|         0.053166336|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('num_days_late','num_weeks_late').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can format the council district column a little differently. We'll add leading 0s to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('council_district', col('council_district').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '%03d' means at least 3 digits, pad with 0s\n",
    "#\n",
    "# In order to use the format_string function the way we are, we'll need to\n",
    "# convert council_district back to an integer temporarily, but the final output\n",
    "# will be a string.\n",
    "\n",
    "df = df.withColumn(\n",
    "    'council_district',\n",
    "    format_string('%03d', col('council_district').cast('int'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|council_district|\n",
      "+----------------+\n",
      "|             005|\n",
      "|             003|\n",
      "|             003|\n",
      "|             003|\n",
      "|             007|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('council_district').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Features\n",
    "\n",
    "Let's now create some new features based on our existing data.\n",
    "\n",
    "We will first extract the zipcode from the address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  el paso st,...|\n",
      "|2215  goliad rd, ...|\n",
      "|102  palfrey st w...|\n",
      "|114  la garde st,...|\n",
      "|734  clearview dr...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('request_address').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('zipcode', regexp_extract('request_address', r'\\d+$',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|zipcode|\n",
      "+-------+\n",
      "|  78207|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78228|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('zipcode').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined the zipcode as the last sequence of digits at the end of the string.\n",
    "\n",
    "Next we will create several new, related columns:\n",
    "- `case_age`: How old the case is; difference in days between when the case opened and the current day\n",
    "- `days_to_closed`: The number of days between when the case was opened and when it was closed\n",
    "- `case_lifetime`: Number of days between when the case was opened and when it was closed, if the case is still open, the number of days since the case was opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.withColumn(\n",
    "        'case_age', datediff(current_timestamp(), 'case_opened_date')\n",
    "    )\n",
    "    .withColumn(\n",
    "        'days_to_closed', datediff('case_closed_date', 'case_opened_date')\n",
    "    )\n",
    "    .withColumn(\n",
    "        'case_lifetime',\n",
    "        when(expr('! case_closed'), col('case_age')).otherwise(\n",
    "            col('days_to_closed')\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|   case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|       true|2018-01-01 00:42:00|2018-01-01 12:29:00|     729|             0|            0|\n",
      "|       true|2018-01-01 00:46:00|2018-01-03 08:11:00|     729|             2|            2|\n",
      "|       true|2018-01-01 00:48:00|2018-01-02 07:57:00|     729|             1|            1|\n",
      "|       true|2018-01-01 01:29:00|2018-01-02 08:13:00|     729|             1|            1|\n",
      "|       true|2018-01-01 01:34:00|2018-01-01 13:29:00|     729|             0|            0|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    'case_closed',\n",
    "    'case_opened_date',\n",
    "    'case_closed_date',\n",
    "    'case_age',\n",
    "    'days_to_closed',\n",
    "    'case_lifetime',\n",
    ").where(expr('case_closed')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+----------------+--------+--------------+-------------+\n",
      "|      false|2018-01-02 09:39:00|            null|     728|          null|          728|\n",
      "|      false|2018-01-02 10:49:00|            null|     728|          null|          728|\n",
      "|      false|2018-01-02 13:45:00|            null|     728|          null|          728|\n",
      "|      false|2018-01-02 14:09:00|            null|     728|          null|          728|\n",
      "|      false|2018-01-02 14:34:00|            null|     728|          null|          728|\n",
      "+-----------+-------------------+----------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    'case_closed',\n",
    "    'case_opened_date',\n",
    "    'case_closed_date',\n",
    "    'case_age',\n",
    "    'days_to_closed',\n",
    "    'case_lifetime',\n",
    ").where(expr('! case_closed')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Department Data\n",
    "\n",
    "We have access to anotehr dataset, `dept.csv`, that contains more information about the various departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = spark.read.csv('data/dept.csv', header = True, inferSchema = True)\n",
    "dept.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to include  this data, so we can join it to our case dataframe using the `dept_division` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    # Left join on dept_division\n",
    "    .join(dept, 'dept_division', 'left')\n",
    "    # drop all the columns except for the standardized name, as it has much fewer unique values\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .drop(df.dept_division)\n",
    "    .withColumnRenamed('standardized_dept_name', 'department')\n",
    "    # convert to a boolean\n",
    "    .withColumn('dept_subject_to_SLA', col('dept_subject_to_SLA') == 'YES')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 12:29:00  \n",
      " case_due_date        | 2020-09-26 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  el paso st,... \n",
      " council_district     | 005                  \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 729                  \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 2018-01-01 00:46:00  \n",
      " case_closed_date     | 2018-01-03 08:11:00  \n",
      " case_due_date        | 2018-01-05 08:30:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | true                 \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  goliad rd, ... \n",
      " council_district     | 003                  \n",
      " num_weeks_late       | -0.28751488099999994 \n",
      " zipcode              | 78223                \n",
      " case_age             | 729                  \n",
      " days_to_closed       | 2                    \n",
      " case_lifetime        | 2                    \n",
      " department           | Trans & Cap Impro... \n",
      " dept_subject_to_SLA  | true                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[case_id: int, case_opened_date: timestamp, case_closed_date: timestamp, case_due_date: timestamp, case_late: boolean, num_days_late: double, case_closed: boolean, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string, num_weeks_late: double, zipcode: string, case_age: int, days_to_closed: int, case_lifetime: int, department: string, dept_subject_to_SLA: boolean],\n",
       " DataFrame[case_id: int, case_opened_date: timestamp, case_closed_date: timestamp, case_due_date: timestamp, case_late: boolean, num_days_late: double, case_closed: boolean, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: string, num_weeks_late: double, zipcode: string, case_age: int, days_to_closed: int, case_lifetime: int, department: string, dept_subject_to_SLA: boolean])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2])\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = df.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
